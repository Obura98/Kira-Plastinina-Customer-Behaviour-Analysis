---
title: "Kira-Plastinina-Customer-Behaviour-Analysis"
author: "Bill"
date: "11/5/2020"
output:
  html_document: default
  pdf_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# 1. Problem Definition
## 1.1 Defining the Question
Kira Plastinina  is a Russian brand that is sold through a defunct chain of retail stores in Russia, Ukraine, Kazakhstan, Belarus, China, Philippines, and Armenia. The brand’s Sales and Marketing team would like to understand their customer’s behavior from data that they have collected over the past year.
## 1.2 Specifying the Question
Create a model that helps KIra Plastinina  learn the characteristics of customer groups
## 1.3  Defining the Metric of Success
- model that predicts accurately with an accuracy score of 80% whether a user will click on an ad or not
- Give insights to the company concerning their brand from the EDA done
- Challenge and improve the created unsupervied model
## 1.4 Understanding the Context
 Kira Plastininais a Russian fashion designer and entrepreneur. Her brand was sold through a now defunct chain of eponymous retail stores in Russia, Ukraine, Kazakhstan, Belarus, China, Philippinesand Armenia
Plastinina was born in Moscow. Her father, Sergei Plastinin, founded Wimm-Bill-Dann Foods OJSC in 1992 and served as its Chief Executive Officer until April 3, 2006. He was impressed with his daughter's passion for design and vision of teenage fashion, and suggested that they launch a fashion brand together.In 2007, the first Kira Plastinina store opened in Moscow, Plastinina introduced her first collection and became one of the youngest fashion designers in the world. Since then, the company has opened over 300 stores in Russia and CIS. In 2008, the Company made an unsuccessful attempt to enter the U.S. market. The U.S. entity eventually filed for bankruptcy. With out analysis segmentation, we can identify why her brand failed in the US maybe help her in her customer segmentation

## 1.5 Experimental Design taken
1. Data Exploration
2. Data Cleaning and Formatting
3. Univariate Analysis
4. Bivariate Analysis
5. Multivariate Analysis
6. Implementing the solution through unsupervised machine learning,i.e. k-means, hierachical and DBSCAN
6. Conclusion and Next steps
# 2. Data Sourcing
The data was availed to our data science team by the  brand’s Sales and Marketing team therefore no data collection and scrapping was needed...We will just load our dataset in RStudio and begin the analysis process
# 3. Check the Data
``` {r}
## Loading packages that we will use during our analysis
library("dplyr")
library("purrr")
library('tidyverse')
library('magrittr')
library('corrplot')
library('caret')
library('skimr')
library(readr)

```

``` {r}
customer_behavior<- read.csv("dataset/online_shoppers_intention.csv")
## previewing first 6 rows
head(customer_behavior)

##previewing the last 6 rows of the dataset
tail(customer_behavior)
```

``` {r}
## Previewing the shape of our dataset
dim(customer_behavior)
### we have 12330     rows and 18 columns!!!BAM!!

#checking the datatypes on the columns 
sapply(customer_behavior, class)
 ## The dataset consists of 10 numerical and 8 categorical attributes.

##checking for structure is using the str()
str(customer_behavior)

## We then a statistical summary of our dataset

summary(customer_behavior)

```
## Appropriateness of Our Dataset
Though there are lots Of NA's present, we will handle them during data cleaning..The dataset contains 12330 rows and 18 columns which is sufficient enough for modelling. Below is a description of the dataset columns

- "Administrative", "Administrative Duration", "Informational", "Informational Duration", "Product Related" and "Product Related Duration" represents the number of different types of pages visited by the visitor in that session and total time spent in each of these page categories. The values of these features are derived from the URL information of the pages visited by the user and updated in real-time when a user takes an action, e.g. moving from one page to another. 
- The "Bounce Rate", "Exit Rate" and "Page Value" features represent the metrics measured by "Google Analytics" for each page in the e-commerce site. 
- The value of the "Bounce Rate" feature for a web page refers to the percentage of visitors who enter the site from that page and then leave ("bounce") without triggering any other requests to the analytics server during that session. 
- The value of the "Exit Rate" feature for a specific web page is calculated as for all pageviews to the page, the percentage that was the last in the session.
- The "Page Value" feature represents the average value for a web page that a user visited before completing an e-commerce transaction. 
- The "Special Day" feature indicates the closeness of the site visiting time to a specific special day (e.g. Mother’s Day, Valentine's Day) in which the sessions are more likely to be finalized with the transaction. The value of this attribute is determined by considering the dynamics of e-commerce such as the duration between the order date and delivery date. For example, for Valentina’s day, this value takes a nonzero value between February 2 and February 12, zero before and after this date unless it is close to another special day, and its maximum value of 1 on February 8. 
- The dataset also includes the operating system, browser, region, traffic type, visitor type as returning or new visitor, a Boolean value indicating whether the date of the visit is weekend, and month of the year.


# 4. Perform Data Cleaning
## To ensure uniformity, I will lowercase all the columns
``` {r}
names(customer_behavior)<- tolower(names(customer_behavior))
head(customer_behavior) 


```
Change has been effeccted

NEXT I'm going to checking for missing values in our dataset,,Missing values may affect the perfomance of our model, so we will find a way to deal with them

``` {r}
##Checking for missing values in each row
colSums(is.na(customer_behavior))

```

We have 14  missing  values in the columns administrative, administrative_duration, information, information_duration, productrelated, productrelated_duration, bouncerates ,exitrates              
Lets see how we will deal with the missing values
``` {r}
#### I will try and omit the missing values and see if it will affect out dataframe,
#### To do this, I will create a temporary dataframe
temp_df<- customer_behavior

temp_df <- na.omit(temp_df)

colSums(is.na(temp_df))
##### lets check for the shape of our dataset
dim(temp_df)






```
At first we had 12330 rows and 18 columns, after dropping missing values, we now have 12316 rows and 18 columns,,,This change is so small and it cannot affect our dataset.

I will now apply this change to our main customer behavior dataset
``` {r}
customer_behavior <- na.omit(customer_behavior)
colSums(is.na(customer_behavior))



```

So far so good!!! Lets now check for duplicates in our dataset,,,This may arise due to mistakes incurred during data collection and data entry.. R gives us better ways of dealing and checking duplicates,,,Lets explore them below

``` {R}
duplicated_rows<- customer_behavior[duplicated(customer_behavior),]

duplicated_rows
## This is awe-inspiring, we also dont have duplicated,,the data collection team should be congratulated


```
We duplicated rows in our dataset,,,We will remove them

``` {r}
customer_behavior_new<- unique(customer_behavior)

##Lets confirm if change has been effected
# confirming from the data for any duplicated records
anyDuplicated(customer_behavior_new)


```

Next we are going to check for outliers in our numerical data,,This can be very extremely high or low values in our dataset that needs investigation...We only check for outliers in our numerical columns

``` {r}
## obtaining numerical columns
numeric_columns <- unlist(lapply(customer_behavior_new, is.numeric))

numeric_columns
## I will put the numerical columns in a dataframe

columns_numeric <- customer_behavior_new[ , numeric_columns]

head(columns_numeric)

```

```{r}
# using a for lop, I will output boxplots of numerical columns..This will help me to identify the outliers

par ( mfrow= c (  2, 4 ))
for (i in 1 : length (columns_numeric)) {
boxplot (columns_numeric[,i], main= names (columns_numeric[i]), type= "l" )
}
```
We have several outliers in our numerical columns but removing them will bring an issue in our modelling so we will just keep them but if need arises in challenging our solution, we will remove them.

##Next We will check anomalies and inconsistenicies in our dataframe....Since we have categorical column, we will need to convert them to factors

``` {r}
lengths(lapply(customer_behavior_new, unique))

customer_behavior_new$revenue  <- as.factor(customer_behavior_new$revenue)
customer_behavior_new$visitortype<-as.factor(customer_behavior_new$visitortype)
customer_behavior_new$weekend<- as.factor(customer_behavior_new$weekend)
customer_behavior_new$specialday<- as.factor(customer_behavior_new$specialday)
customer_behavior_new$month<- as.factor(customer_behavior_new$month)
customer_behavior_new$region<- as.factor(customer_behavior_new$region)



```

``` {r}
## checking if change has been effected
str(customer_behavior_new)



```

## Observations
- Revenue has 2 levels, signifying if revenue was made or not
- weekend has 2 levels signifying if the it was a weekend or not
- visitor type has 3 levels new_visitor,returning_visitor and others
- We also have 9 regions in our dataset

# 5. Perform Exploratory Data Analysis  (Univariate, Bivariate & Multivariate)

## Univariate Analysis
Here we analyse single variables by checking the measures of central tendency and measures of dispersion...
Lets begin with measures of central tendency 
``` {r}
## Getting the mean of all numerical columns
colMeans(columns_numeric)

```
##Median
``` {r}
apply(columns_numeric,2,median)

```

### Mode
``` {r}
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}



administrative_mode <- getmode(customer_behavior_new$administrative)
print(administrative_mode)
##This page was not frequently visited by many people as the mode is 0

administrative_duration_mode<- getmode(customer_behavior_new$administrative_duration)
print(administrative_duration_mode)

## Most users did not spend time in this page as the mode is 0
informational_mode <- getmode(customer_behavior_new$informational)
print(informational_mode)
##This page was not frequently visited by many people as the mode is 0

informational_duration_mode <- getmode(customer_behavior_new$informational_duration)
print(informational_duration_mode)
##Most users did not spend more time on this page as the duration mode was 0

productrelated_mode <- getmode(customer_behavior_new$productrelated)
print(productrelated_mode)
## Many people visited the products page as it had a mode of 1

productrelated_duration_mode <- getmode(customer_behavior_new$productrelated_duration)
print(productrelated_duration_mode)
## Though many visited this page, they however did not spend more time on the page

bouncerates_mode <- getmode(customer_behavior_new$bouncerates)
print(bouncerates_mode)

## The bounce rate being zero indicates that more people who visited the page triggered a request

exitrates_mode <- getmode(customer_behavior_new$exitrates)
print(exitrates_mode)

## The frequent pageview was 20% to each page
operatingsystems_mode <- getmode(customer_behavior_new$operatingsystems)
print(operatingsystems_mode)

region_mode <- getmode(customer_behavior_new$region)
print(region_mode)

traffictype_mode <- getmode(customer_behavior_new$traffictype)
print(traffictype_mode)

pagevalue_mode <- getmode(customer_behavior_new$pagevalues)
print(pagevalue_mode)

## the average value for a web page that a user visited before completing an e-commerce transaction was 0




```
##Measures of Dispersion

```{r}
## we will do this by finding the statistical summary
summary(customer_behavior_new)


```
## Standard Deviation
``` {r}
apply(columns_numeric,2,sd)

```
## Variance
``` {r}

sapply(columns_numeric, var)

```
### Histograms
```{r}
par(mfrow=c(2, 4))
for (i in 1:length(columns_numeric)) {
        hist(columns_numeric[,i], main=names(columns_numeric[i]))
}
```

## Bivariate Analysis
I will check on the relationship between our dependent variable and other columns
``` {r}
customer_behavior_new
# Lets see the revenue that the administrative page obtained
administrative_rev <- table(customer_behavior_new$revenue, customer_behavior_new$administrative)

names(dimnames(administrative_rev)) <- c("revenue", "administrative")
administrative_rev

```
## The administrative page did not accrue alot of revenue as there are many FALSE than true

# Lets see the revenue that the administrative page obtained

``` {r}
## Lets investigate the revenue incurred in the informational page

informational_rev <- table(customer_behavior_new$revenue, customer_behavior_new$informational)

names(dimnames(informational_rev)) <- c("revenue", "informational")
informational_rev

```

## The revenue accrued on the informational page was average but it was pretty higher than that of the administrative page


``` {r}
## Lets investigate the revenue incurred in the productrelated page
productrelated_rev <- table(customer_behavior_new$revenue, customer_behavior_new$productrelated)

names(dimnames(productrelated_rev)) <- c("revenue", "productrelated")
productrelated_rev


```

## The product related page also did not accrue alot of revenue as there many falses than rows...The dataset seems imbalanced as the values of falses are many as compared to Trues
``` {r}
## Lets investigate the revenue accrued per visitortype

visitor_rev <- table(customer_behavior_new$revenue, customer_behavior_new$visitortype)

names(dimnames(visitor_rev)) <- c("revenue", "visitor_rev")
visitor_rev

customer_behavior_new

```
##Returning Visitors brought more revenue to the brand and others brought the minimum revenue to the brand
``` {r}
## Lets investigate the revenue accrued per month

month_rev <- table(customer_behavior_new$revenue, customer_behavior_new$month)

names(dimnames(month_rev)) <- c("revenue", "month")
month_rev

customer_behavior_new

```

## On the month of November, The brand received more revenue and in the Month of february, the brand received the least revenue
``` {r}
## Lets investigate the revenue accrued per region

region_rev <- table(customer_behavior_new$revenue, customer_behavior_new$region)

names(dimnames(region_rev)) <- c("revenue", "region")
region_rev



```
## On region 1 the brand got more revenue of 771 and the least region to give the brand revenue was region 5....I guess that is china...China Vs Russia war of supremacy


``` {r}

## Lets see the revenue accrued on weekends
weekend_rev <- table(customer_behavior_new$revenue, customer_behavior_new$weekend)

names(dimnames(region_rev)) <- c("revenue", "weekend")
weekend_rev

```
## The brand received more revenue on weekdays than on weekends

##Correlation matrix of all numerical columns
``` {r}

correlations <- cor(columns_numeric, method = "pearson")

round(correlations, 2)

```
``` {r}

library('corrplot')
corrplot(correlations, type = "lower", order = "hclust",tl.col = "black", tl.srt = 40)
```


# 6. Implement the Solution
### Unsupervised learning models
``` {r}
##K-Means Clustering
# Since clustering is a type of Unsupervised Learning, we would not require Class Labelduring execution of our algorithm. 
# We will, therefore, remove Class Attribute “Revenue” and store it in another variable. 
# We would then normalize the attributes between 0 and 1 using our own function.
customer_behavior_new
customer<- customer_behavior_new[,c(1,2,3,4,5,6,7,8,9)]
head(customer)
# Normalizing the dataset so that no particular attribute has more impact on clustering algorithm than others.
normalize <- function(x){
  return ((x-min(x)) / (max(x)-min(x)))
}
##normalizing our columns
customer$administrative<- normalize(customer$administrative)
customer$administrative_duration<- normalize(customer$administrative_duration)
customer$informational<- normalize(customer$informational)
customer$informational_duration<- normalize(customer$informational_duration)
customer$productrelated<- normalize(customer$productrelated)
customer$productrelated_duration<- normalize(customer$productrelated_duration)
customer$bouncerates<- normalize(customer$bouncerates)
customer$exitrates<- normalize(customer$exitrates)
customer$pagevalues<- normalize(customer$pagevalues)

summary(customer)
##WE can see that all our minimums are 0 and maximum is 1.....Our columns are ready for clustering

```

``` {r}
# Applying the K-means clustering algorithm with no. of centroids(k)=3
output<- kmeans(customer,3) 
# Previewing the no. of records in each cluster
# 
output$size 
# Getting the value of cluster center datapoint value(3 centers for k=3)
# ---
# 
output$centers 
# Getting the cluster vector that shows the cluster where each record falls
# ---
# 
output$cluster



```
``` {r}
# Visualizing the  clustering results
# ---
# 
par(mfrow=c(1,2), mar=c(5,4,2,2))
plot(customer[,1:2], col=output$cluster) ##plot to see how administrative and administrativeduration have been clustered

```
- This graph shows that Wind and Temp data points have not been clustered properly. Let us find out which attributes have been taken into consideration more by k-means algorithm. For this, we will plot all possible combinations of attributes!


``` {r}
plot(customer[,], col=output$cluster) # Plot to see all attribute combinations


```
From the above plot, it can be seen that k-means algorithm has successfully clustered the columns but it is not that good,,,lets examine the hierachical clustering...

``` {r}
# As we don’t want the hierarchical clustering result to depend to an arbitrary variable unit, 
# we start by scaling the data using the R function scale() as follows

customer_h<- customer_behavior_new[,c(1,2,3,4,5,6,7,8,9)]
head(customer_h)
customer_h <- scale(customer_h)
head(customer_h)

```
``` {r}
# First we use the dist() function to compute the Euclidean distance between observations, 
# d will be the first argument in the hclust() function dissimilarity matrix
# ---
d <- dist(customer_h, method = "euclidean")
# We then hierarchical clustering using the Ward's method
# ---
# 
res.hc <- hclust(d, method = "ward.D2" )

# Lastly, we plot the obtained dendrogram
# ---
# 
plot(res.hc, cex = 0.6, hang = -1)
```

``` {r}
## Advanced Hierachical Clustering

library(ggplot2)
library(dplyr)
library(dendextend)
library(factoextra)
library(cluster)
#Determine optimal number of clusters
#We will use fviz_nbclust() method to check optimum number of clusters using silhouette, wss and gap_stat.
# Method 1 - Silhouette
#Clustering
head(customer_h)
set.seed(123)

#Determining the number of optimal clusters 
#Determining optimal number of Clusters (Cluster Evaluation Method 1)

fviz_nbclust(customer_h, FUN = hcut, method = "silhouette")




```
###The optimum number of clusters is 3....Nice

``` {r}
##Create distance matrix
#We now use manhattan distance formula to create a distance matrix. Using Manhattan distance, the silhouette plots obtained were better with higher co-efficient. Hence, we have used Manhattan distance.

#calculate manhattan distance
data2di <- dist(customer_h, method = "man")
##Now that we have created our distance matrix we can create our hierarchical cluster with optimal number of clusters as 3.

#Method 1 - Complete linkage
#complete
data2hc <- hclust(data2di, method = "complete")

data2as <- cutree(data2hc, k = 3)

dend_data <- as.dendrogram(data2hc)

cc <- color_branches(dend_data, k=3)

plot(cc)


```

``` {r}
sil <- silhouette(data2as, data2di)

fviz_silhouette(sil,palette= "jco",ggtheme = theme_minimal())

```
## As seen in the plot above the average silhouette score is higher and only the grey cluster shows a minimal negative score. The negative score denotes a few observations are not in the right cluster.

``` {r}
##Method 2 - Single linkage
#single

data2hc <- hclust(data2di, method = "single")
data2as <- cutree(data2hc, k = 3)

dend_data <- as.dendrogram(data2hc)
cc <- color_branches(dend_data, k=3)
plot(cc)

sil <- silhouette(data2as, data2di)
fviz_silhouette(sil,palette= "jco",ggtheme = theme_minimal())
```
##As seen in the plot above the average silhouette score is lower and most of the observations have gone into the blue cluster. There is also a high negative score which denotes a a large number of observations are not in the right cluster.

``` {r}

##Method 3 - Average linkage

#average

data2hc <- hclust(data2di, method = "average")
data2as <- cutree(data2hc, k = 3)

dend_data <- as.dendrogram(data2hc)
cc <- color_branches(dend_data, k=3)
plot(cc)

sil <- silhouette(data2as, data2di)
fviz_silhouette(sil,palette= "jco",ggtheme = theme_minimal())


```

##As seen in the plot above the average silhouette score is higher and only the grey cluster shows a minimal negative score. The negative score denotes a slight misclassification. This means they are not in the right cluster.
 
 

## 7. Challenge the Solution


```{r}
# We challenge the solution using DBSCAN algorithm to see if it performs better clustering
# Loading necessary libraries
pacman :: p_load(dbscan)

# obtaining optimal nearest neighbours
kNNdistplot(customer_behavior_new[,1:9],k=3) 
# shows optimal distance at approx 2000 for k value which we already know as 2 based on revenue class
```
```{r}
# We want minimum 2 Cluster points with in a distance of eps(2000)
# 
output_df <- dbscan(customer_behavior_new[,1:9],eps=2000,MinPts = 2, borderPoints = TRUE)
output_df
```

```{r}
# plot clusters using hullplot()
hullplot(customer_behavior_new[,1:9],output_df$cluster)
```
 
 
 
 ## Conclusions
 1. The K_Means gave us 3 clear clusters while the Hierarchical clustering model, clusters were crowded
 2. Because our dataset was large, K-Means may be computationally faster than hierarchical clustering (if K is small). 
 4. Hierarchical clustering outputs a hierarchy, ie a structure that is more informative than the unstructured set of flat clusters returned by k-means.





## To challenge our solution, we are going to use the DBSCAN clustering method to challenge our solution
# 8. Follow up Questions
- We had the right data though the dataset was imbalanced
- MAybe if I dealt well with outliers, then clustering would be perfect


